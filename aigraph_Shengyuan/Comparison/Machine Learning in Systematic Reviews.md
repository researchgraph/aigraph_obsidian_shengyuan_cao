#Comparison
## Research Objectives
- **Aim 1**: Assess the usefulness of an automated clustering method, Lingo3G, in categorizing studies for a simplified rapid review.
- **Aim 2**: Compare the performance (precision and recall) of automated clustering with manual categorization by human researchers.

## Clustering Methods
- Human-created categorization, which includes:
  - Researcher blinded to cluster assignment
  - Researcher non-blinded to cluster assignment
- Automated clustering using Lingo3G

## Evaluation Metrics
- Time use
- Precision
- Recall

## Experimental Results
- The study was evaluated on 128 randomly assigned studies.
1. **Topic Categories**: Automated clusters did not correspond to two important manual categories: country and study design.
2. **Topic/Findings**: Most qualitative studies were categorized under "life experiences, lived experiences" and "negative experiences". Quantitative studies were mostly categorized under "factors related to criminality/desistance".
3. **Precision Rates**: In automated clustering, the precision rates were comparable to that of a human researcher (81%-82%).
4. **Time Use**: Manual categorization took 11.4 hours, while automated clustering took 7.7 hours for the same 128 studies.

## Conclusions and Issues

### Human-created Categorization Benefits
1. Can create complex categorization systems.
2. Potentially more trusted.
3. Can identify knowledge gaps or empty categories.

### Human-created Categorization Limitations
1. Time-consuming to establish and validate the system.

### Automated Clustering Benefits
1. Quick cluster creation.
2. No need for pilot testing.
3. Comparable performance to human researchers.
4. Highlights range of clusters.
5. May capture topics not identified by researchers.
6. Flexible settings.

### Automated Clustering Limitations
1. Cannot adhere to a pre-defined categorization system.
2. Requires time and interpretation for sense-making.

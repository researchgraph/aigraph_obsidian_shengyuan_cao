This report provides a comprehensive analysis of the text preprocessing part of current 4 clustering methods, giving both individual and summarized suggestions and analysis.

## 1. Overview

Generally, all of our pre-processing parts have similar goals and components. The goals include:

- Text cleaning.
- Tokenization.
- Lemmatization.
- Term frequency analysis.

## 2. Code Content Analysis

### [[Aishwarya's part]]
#### a. Data Cleaning and Preprocessing:

Utilizing the `gensim` library, the following preprocessing steps are applied to the text:

- Convert to lowercase.
- Remove HTML tags.
- Strip punctuation.
- Eliminate excess white spaces.
- Remove numbers.
- Omit non-alphanumeric characters.
- Exclude words shorter than 4 characters.

#### b. Lemmatization:

With the use of `nltk`'s `WordNetLemmatizer`, words in the text are reduced to their base form.

#### c. Stopwords Removal:

Both `gensim` and `nltk` library are employed to identify and eliminate stopwords. Beyond default stopwords, there's a custom list of stopwords tailored for this application. About 100 stopwords are listed.
#### d. Advantages:

- The code covers an extensive range of text preprocessing steps, from basic data cleaning to lemmatization and stopwords removal.
- The use of specialized stopwords list aids in excluding irrelevant terms for a specific application.
#### e. Disadvantages:

- Code readability and modularization can be further enhanced. For instance, preprocessing, lemmatization, and stopwords removal could be segmented into individual functions, like Shengyuan's implementation.
- The tailored stopwords list is a bit long and might risk omitting certain significant terms, especially when some words have multiple meanings.

#### f. Recommendations:

- Consider dividing preprocessing, lemmatization, and stopwords removal into separate functions to enhance code readability and reusability.
- For the specialized stopwords list, it's advisable to inspect its efficacy post-model training, ensuring no vital information gets unintentionally discarded.
### [[Shuyi's part]]

#### a. Lemmatization:

- The `WordNetLemmatizer` from NLTK was utilized for lemmatization.
- Before lemmatizing, POS (Part of Speech) tagging was used to determine the grammatical categry of each word, allowing for more accurate lemmatization.

#### b. Text Cleaning:

- All HTML tags were removed.
- Only words containing alphabets were retained.

#### c. Stopword Removal:

- NLTK's `stopwords` was used to remove English stopwords.

#### d. Term Frequency Calculation:

- Each word was counted and stored in a dictionary named `terms_dict`.

#### e. Removal of High and Low-frequency Words:

- The top 50 most frequent words from `terms_dict` were selected.
- Words that appeared only once were also identified.
- Both categories of words were added to an `avoiding_words` list for removal in subsequent steps.
#### f. Advantages:

- Using POS tagging for lemmatization is impressive as it enhances accuracy effectively.
- Deleting high and low-frequency words can be beneficial for clustering since such words might not carry much valuable information.

#### g. Disadvantages:

- Compared to the first code, this segment is relatively simpler in terms of stopword removal.
- Arbitrarily removing the top 50 frequent terms might not always be optimal, potentially discarding crucial terms. Although the risk of over-processing is not on stopwords here, it still raises the potential issue using the frequency-oriented technique.

#### h. Recommendations:

- Consider adopting a dynamic method to determine the number of high-frequency words to remove, possibly based on a percentage of word frequency.
- Encapsulate preprocessing, cleaning, lemmatization, etc. into separate functions to augment code modularization and normalization.
### [[Shengyuan's part]]

#### a. Text Cleaning:

- **clean_text_round1**: This function removes HTML tags and escape characters from a given text.

#### b. Noun Extraction and Lemmatization:

- **nouns**: This function tokenizes the text and focuses only on nouns. I also used POS tagging to identify nouns, lemmatizes them, and then returns the lemmatized nouns as a single string.

#### c. Vectorization:

- A custom list of stop words (`stop_noun`) is created like Aishwarya, but with fewer words.
- TF-IDF Vectorization is applied on the abstracts that now consist of only lemmatized nouns. The vectorizer consider bigrams, and both custom and standard English stop words.

#### d. Topic Modeling Helper:

- **display_topics**: This function displays topics generated by a model. It considers potential issues such as duplicate bigrams and single terms that might be components of bigrams.
#### e. Advantages

- I Implemented modularization through function encapsulation. 
- Additionally, I introduced extra functions to handle potential redundancies arising from bigram representations.
#### f. Disadvantages

- **lack_of_standards**: The stopwords are not generated by certain rules.
- **redundant_pre_processing**: Dealing with bigram problems is not an universal solution especially for other codes without bigram processes. This should be noted when figuring out the unifed version of pre-processing.
#### g. Recommendations:

1. **Refactoring**: The function `nouns` mixes tokenization, POS tagging, and lemmatization. You might consider breaking this down for clarity.
    
2. **Error Handling**: Consider adding some error handling or checks. For instance, the regex used in the `clean_text_round1` function assumes well-formed HTML tags, but what if they aren’t? Similar issues are raised during the first general meeting.
    
3. **TF-IDF Parameters**: You've set `max_df` and `min_df` parameters in the `TfidfVectorizer`. It’s good to experiment with these, perhaps using cross-validation, to get the best values.
### [[Yunzhong's part]]

#### a. Data Cleaning and Preprocessing:

Utilizing the `nltk` library, the following preprocessing steps are applied to the text:

- Remove HTML tags.
#### b. Lemmatization:

With the use of `nltk`'s `WordNetLemmatizer`, words in the text are reduced to their base form.
#### c. Stopwords Removal:

Just like Aishwarya's implementation, there's a custom list of stopwords tailored for this application. About 100 stopwords are listed.
#### d. Advantages:

- The code covers an extensive range of text preprocessing steps, from basic data cleaning to lemmatization and stopwords removal.
- The use of specialized stopwords list aids in excluding irrelevant terms for a specific application.
- Good modularization and clear code structure.
- Using POS like Shuyi, leading to a great lemmatization.
#### e. Disadvantages:

- The tailored stopwords list is a bit long and might risk omitting certain significant terms, especially when some words have multiple meanings.
#### f. Recommendations:

- For the specialized stopwords list, it's advisable to inspect its efficacy post-model training, ensuring no vital information gets unintentionally discarded.
## 3. Summary

- The pre-processing of current codes are generally similar but have very different details. Hereby, I have listed my understanding of the construction of the unified version of pre-processing.
### a. Applying unified stopwords:

- Current codes have multiple versions of stopwords with respect to various rules. I suggest that we all apply the same stopwords based on specific rules, which can be discussed over the meeting.
### b. Introducing stemming:

- All the codes are not considering stemming. For instance, the necessity of Handling Abbreviations and Acronyms.
### c. POS Tagging

- If necessary, tag words with their parts of speech. This can be particularly helpful if further processing steps or analyses require understanding the grammatical role of words in sentences.
### d. Handling Redundancies in Bigrams

- Some of the codes are not considering bigrams. Bigrams are necessary especially in AI fields, like COVID 19, Natural Language, etc., so I sugget we all consider bigrams processing and the potential issue of introducing which, like redundancies I encountered.
- Implement strategies to detect and eliminate redundancies in bigram representations to ensure meaningful and distinct feature extraction.